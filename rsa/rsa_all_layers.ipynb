{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSA\n",
    "\n",
    "This notebook has code to compute RSA between Exp48/fMRI data and model representations. Spearman correlations are computed between model representations and fMRI responses aggregated across participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from src.paths import ROOT\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from src.fmri_rsa_utils import *\n",
    "from src.utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_results = open_json(ROOT / 'results/rsa/rsa_brain_set2.json')\n",
    "exp48_results = open_json(ROOT / 'results/rsa/rsa_exp48_set2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating RDMs for Exp features\n",
    "\n",
    "# experiential features\n",
    "exp_ftrs = pd.read_csv(ROOT / 'data/exp_features.csv')\n",
    "\n",
    "# organising exp arrays into a dict\n",
    "exp_dict = {exp_ftrs.iloc[i,0]: np.array(exp_ftrs.iloc[i,1:].tolist()) for i in range(len(exp_ftrs))}\n",
    "\n",
    "# distance matrices (1-cosine_similarity)\n",
    "s2_exp = pd.read_csv(ROOT / 'data/Study2_model_RSMs/Exp48_SOE320_sim_mat.csv')\n",
    "words_s2 = s2_exp.Word.tolist()\n",
    "\n",
    "# organising words from study 2 in a dedicated matrix\n",
    "s2_exp_mat = np.stack([exp_dict[word] for word in words_s2])\n",
    "\n",
    "# obtaining RDM\n",
    "exp_rdm2 = get_distance_vec(s2_exp_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading brain data into RDM\n",
    "brain_2 = get_fmri_rdm_study2_aggregated('semantic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolated words\n",
    "\n",
    "Here, we're considering embeddings obtained by inputting the models with isolated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse = pickle.load(open(ROOT / 'results/simcse/simcse_layers.pkl', 'rb'))\n",
    "mcse = pickle.load(open(ROOT / 'results/mcse/mcse_bert_coco_layers.pkl', 'rb'))\n",
    "clap = pickle.load(open(ROOT / 'results/clap/clap_layers.pkl', 'rb'))\n",
    "clip = pickle.load(open(ROOT / 'results/clip/clip.pkl', 'rb'))\n",
    "bert = pickle.load(open(ROOT / 'results/bert/bert_layers.pkl', 'rb'))\n",
    "vbert = pickle.load(open(ROOT / 'results/visualbert/visualbert_layers.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_embs, model_name in zip([simcse, mcse, clap, bert, vbert, clip], ['simcse', 'mcse', 'clap', 'bert', 'visualbert', 'clip']):\n",
    "    brain_alignment = compute_layer_alignment_w_brain_aggregated(model_embs, brain_2, words_s2)\n",
    "    exp48_alignment = compute_layer_alignment_w_exp48(model_embs, exp_rdm2, words_s2)\n",
    "    brain_results[model_name]['non-contextualised']['set2'] = brain_alignment\n",
    "    exp48_results[model_name]['non-contextualised']['set2'] = exp48_alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing additional baselines with GloVe and Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2 = pd.read_csv(ROOT / 'data/Study2_model_RSMs/GloVe_SOE320_sim_mat.csv')\n",
    "\n",
    "glove2_rdm = np.array(1 - glove2.iloc[:,1:])[np.triu_indices(n=320, m=320, k=1)]\n",
    "\n",
    "\n",
    "rho_exp_2, pval_exp_2 = spearmanr(exp_rdm2, glove2_rdm)\n",
    "print(f\"\\nStudy 2 corr w/ Exp48: corr={round(rho_exp_2, 3)} p-val={round(pval_exp_2, 3)}\")\n",
    "\n",
    "rho_brain_2, pval_brain_2 = spearmanr(brain_2, glove2_rdm)\n",
    "print(f\"Study 2 corr w/ Brain: corr={round(rho_brain_2, 3)} p-val={round(pval_brain_2, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2 = pd.read_csv(ROOT / 'data/Study2_model_RSMs/word2vec_SOE320_sim_mat.csv')\n",
    "\n",
    "w2v2_rdm = np.array(1 - w2v2.iloc[:,1:])[np.triu_indices(n=320, m=320, k=1)]\n",
    "rho_exp_2, pval_exp_2 = spearmanr(exp_rdm2, w2v2_rdm)\n",
    "print(f\"\\nStudy 2 corr w/ Exp48: corr={round(rho_exp_2, 3)} p-val={round(pval_exp_2, 3)}\")\n",
    "\n",
    "rho_brain_2, pval_brain_2 = spearmanr(brain_2, w2v2_rdm)\n",
    "print(f\"Study 2 corr w/ Brain: corr={round(rho_brain_2, 3)} p-val={round(pval_brain_2, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualised embeddings\n",
    "\n",
    "Here, we consider embeddings obtained from nouns plugged into neutral sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_cont = pickle.load(open(ROOT / 'results/simcse/simcse_layers_context.pkl', 'rb'))\n",
    "mcse_cont = pickle.load(open(ROOT / 'results/mcse/mcse_layers_context.pkl', 'rb'))\n",
    "clap_cont = pickle.load(open(ROOT / 'results/clap/clap_layers_context.pkl', 'rb'))\n",
    "bert_cont = pickle.load(open(ROOT / 'results/bert/bert_layers_context.pkl', 'rb'))\n",
    "vbert_cont = pickle.load(open(ROOT / 'results/visualbert/visualbert_layers_context.pkl', 'rb'))\n",
    "clip_cont = pickle.load(open(ROOT / 'results/clip/clip_layers_context.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_embs, model_name in zip([simcse_cont, mcse_cont, clap_cont, bert_cont, vbert_cont, clip_cont], ['simcse', 'mcse', 'clap', 'bert', 'visualbert', 'clip']):\n",
    "    avg_mat = average_representations_across_prompts(model_embs)\n",
    "    brain_alignment = compute_layer_alignment_w_brain_aggregated(avg_mat, brain_2, words_s2)\n",
    "    exp48_alignment = compute_layer_alignment_w_exp48(avg_mat, exp_rdm2, words_s2)\n",
    "    brain_results[model_name]['context']['set2'] = brain_alignment\n",
    "    exp48_results[model_name]['context']['set2'] = exp48_alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualised embeddings caption-like templates\n",
    "\n",
    "Here, we consider embeddings obtained from nouns plugged into caption-like sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_cont_vis = pickle.load(open(ROOT / 'results/simcse/simcse_layers_visual_context_specific.pkl', 'rb'))\n",
    "mcse_cont_vis = pickle.load(open(ROOT / 'results/mcse/mcse_layers_visual_context_specific.pkl', 'rb'))\n",
    "clap_cont_vis = pickle.load(open(ROOT / 'results/clap/clap_layers_visual_context_specific.pkl', 'rb'))\n",
    "bert_cont_vis = pickle.load(open(ROOT / 'results/bert/bert_layers_visual_context_specific.pkl', 'rb'))\n",
    "vbert_cont_vis = pickle.load(open(ROOT / 'results/visualbert/visualbert_layers_visual_context_specific.pkl', 'rb'))\n",
    "clip_cont_vis = pickle.load(open(ROOT / 'results/clip/clip_layers_visual_context_specific.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_embs, model_name in zip([simcse_cont_vis, mcse_cont_vis, clap_cont_vis, bert_cont_vis, vbert_cont_vis, clip_cont_vis], ['simcse', 'mcse', 'clap', 'bert', 'visualbert', 'clip']):\n",
    "    avg_mat = average_representations_across_prompts(model_embs)\n",
    "    brain_alignment = compute_layer_alignment_w_brain_aggregated(avg_mat, brain_2, words_s2)\n",
    "    exp48_alignment = compute_layer_alignment_w_exp48(avg_mat, exp_rdm2, words_s2)\n",
    "    brain_results[model_name]['visual context']['set2'] = brain_alignment\n",
    "    exp48_results[model_name]['visual context']['set2'] = exp48_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_json(brain_results, ROOT / 'results/rsa/rsa_brain_set2.json')\n",
    "dict_to_json(exp48_results, ROOT / 'results/rsa/rsa_exp48_set2.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
